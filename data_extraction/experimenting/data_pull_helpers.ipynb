{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/usr/lib/python310.zip', '/usr/lib/python3.10', '/usr/lib/python3.10/lib-dynload', '', '/home/vkhandekar/project_flood/.venv/lib/python3.10/site-packages', '/homes/vk223/ProjectFlood', '/home/vkhandekar/project_flood']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "import ee\n",
    "import cdsapi\n",
    "import xarray as xr\n",
    "import json\n",
    "from shapely.geometry import Polygon, mapping, box\n",
    "import zipfile\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "import os\n",
    "import concurrent.futures\n",
    "import contextlib\n",
    "import rasterio\n",
    "from rasterio.warp import reproject, Resampling\n",
    "# from osgeo import gdal\n",
    "import sys\n",
    "paths_to_add = [\"/homes/vk223/ProjectFlood\", \"/home/vkhandekar/project_flood\"]\n",
    "for p in paths_to_add:\n",
    "    if p not in sys.path:\n",
    "        sys.path.append(p)\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello from the Earth Engine servers!\n"
     ]
    }
   ],
   "source": [
    "ee.Authenticate()\n",
    "ee.Initialize(project='ee-varunkhandekar')\n",
    "print(ee.String('Hello from the Earth Engine servers!').getInfo())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flood Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_overlapping_events(file_path: str):\n",
    "    if file_path.endswith('.csv'):\n",
    "        df = pd.read_csv(file_path)\n",
    "    elif file_path.endswith('.xlsx') or file_path.endswith('.xls'):\n",
    "        df = pd.read_excel(file_path, engine='openpyxl')\n",
    "    else:\n",
    "        raise ValueError(\"Please provide a CSV or Excel file.\")\n",
    "    \n",
    "    # Data file integrity checks\n",
    "    required_columns = ['index', 'dfo_began_uk', 'dfo_ended_uk']\n",
    "    assert all(column in df.columns for column in required_columns), f\"Missing columns: {[column for column in required_columns if column not in df.columns]}\"\n",
    "\n",
    "    df['dfo_began_uk'] = pd.to_datetime(df['dfo_began_uk'], dayfirst=True)\n",
    "    df['dfo_ended_uk'] = pd.to_datetime(df['dfo_ended_uk'], dayfirst=True)\n",
    "\n",
    "    df['overlapping_events'] = ''\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        overlaps = []\n",
    "        for j, compare_row in df.iterrows():\n",
    "            if i != j:\n",
    "                if not (row['dfo_ended_uk'] < compare_row['dfo_began_uk'] or row['dfo_began_uk'] > compare_row['dfo_ended_uk']):\n",
    "                    overlaps.append(j)\n",
    "        df.at[i, 'overlapping_events'] = pd.NA\n",
    "        if len(overlaps) != 0:\n",
    "            df.at[i, 'overlapping_events'] = ','.join(map(str, overlaps))\n",
    "\n",
    "    return df\n",
    "\n",
    "df = generate_overlapping_events('Bangladesh_Flood_Events.xlsx')\n",
    "df.head()\n",
    "len(df.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2001-07-07 00:00:00\n",
      "56\n"
     ]
    }
   ],
   "source": [
    "def generate_flood_events(file_path: str):\n",
    "    # Preprocess raw flood data\n",
    "    df = generate_overlapping_events(file_path)\n",
    "    \n",
    "    overlapping_dict = {}\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        dfo_began_uk = row['dfo_began_uk']\n",
    "        overlapping_events = row['overlapping_events']\n",
    "        \n",
    "        # Create list for overlapping events (list includes the current row itself)\n",
    "        overlapping_events_list = [idx] # add current row to list\n",
    "        if pd.notna(overlapping_events):\n",
    "            overlapping_events_list = [i for i in list(map(int, overlapping_events.split(',')))] # get rows of overlapping events\n",
    "        \n",
    "        overlapping_events_list = [df['index'][i] for i in overlapping_events_list]\n",
    "\n",
    "        # Find earliest event date; \n",
    "        # REMOVE THIS LOGIC IF YOU WANT MORE DATAPOINTS, I.E. NOT MERGING TO THE START DATE OF THE EARLIEST OVERLAPPING EVENT\n",
    "        earliest_event_date = dfo_began_uk\n",
    "        for event in overlapping_events_list:\n",
    "            event_date = df[df['index'] == event]['dfo_began_uk'].values[0]\n",
    "            event_date = pd.Timestamp(event_date)\n",
    "            if event_date < earliest_event_date:\n",
    "                earliest_event_date = event_date\n",
    "        \n",
    "        # Add the events to the dictionary with the earliest event date as key\n",
    "        if earliest_event_date not in overlapping_dict:\n",
    "            overlapping_dict[earliest_event_date] = overlapping_events_list\n",
    "        else:\n",
    "            overlapping_dict[earliest_event_date].extend(overlapping_events_list)\n",
    "            overlapping_dict[earliest_event_date] = list(set(overlapping_dict[earliest_event_date]))\n",
    "    \n",
    "    return overlapping_dict\n",
    "\n",
    "\n",
    "overlapping_dict = generate_flood_events('Bangladesh_Flood_Events.xlsx')\n",
    "soil_moisture_date = pd.to_datetime(list(overlapping_dict.keys())[1])- timedelta(days=1)\n",
    "print(soil_moisture_date)\n",
    "print(len(overlapping_dict.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(88.084422, 20.670883, 92.672721, 26.446526)\n",
      "{'geodesic': False, 'type': 'Polygon', 'coordinates': [[[88.084422, 20.670882999999975], [92.672721, 20.670882999999975], [92.672721, 26.446526000000024], [88.084422, 26.446526000000024], [88.084422, 20.670882999999975]]]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'type': 'Polygon',\n",
       "  'coordinates': (((92.672721, 20.670883),\n",
       "    (92.672721, 26.446526),\n",
       "    (88.084422, 26.446526),\n",
       "    (88.084422, 20.670883),\n",
       "    (92.672721, 20.670883)),)}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shapefile_path = 'bangladesh-outline_68.geojson'\n",
    "# 'bangladesh-detailed-boundary_858.geojson' 'bangladesh-outline_68.geojson'\n",
    "\n",
    "\n",
    "with open(shapefile_path, 'r') as file:\n",
    "    geojson_data = json.load(file)\n",
    "\n",
    "# Extract polygon coordinates\n",
    "polygon_coordinates = []\n",
    "\n",
    "for feature in geojson_data['features']:\n",
    "    if feature['geometry']['type'] == 'Polygon':\n",
    "        polygon_coordinates.append(feature['geometry']['coordinates'])\n",
    "    elif feature['geometry']['type'] == 'MultiPolygon':\n",
    "        for polygon in feature['geometry']['coordinates']:\n",
    "            polygon_coordinates.append(polygon)\n",
    "\n",
    "ee_geometry = ee.Geometry.MultiPolygon(polygon_coordinates)\n",
    "# Create the polygon\n",
    "polygon = Polygon(polygon_coordinates[0][0])\n",
    "\n",
    "# print(ee_geometry.getInfo()['coordinates'])\n",
    "# print(polygon_coordinates[0])\n",
    "# print(ee_geometry.getInfo()['coordinates']==polygon_coordinates[0])\n",
    "print(polygon.bounds)\n",
    "print(ee_geometry.bounds().getInfo())\n",
    "bbox_ee = ee.Geometry.BBox(*polygon.bounds)\n",
    "\n",
    "bbox = box(*polygon.bounds)\n",
    "\n",
    "geojson = [mapping(bbox)]\n",
    "geojson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "era5_soil_moisture = ee.ImageCollection('ECMWF/ERA5_LAND/DAILY_AGGR')\n",
    "soil_moisture = ee.Image(era5_soil_moisture.filterDate(soil_moisture_date.strftime(r\"%Y-%m-%d\"), \n",
    "                                                        (soil_moisture_date+pd.Timedelta(days=1)).strftime(r'%Y-%m-%d')).first())\n",
    "soil_moisture = soil_moisture.select(\"volumetric_soil_water_layer_1\")\n",
    "soil_moisture = soil_moisture.unmask(1)\n",
    "# soil_moisture = soil_moisture.resample(mode=\"bilinear\")\n",
    "export_task = ee.batch.Export.image.toDrive(\n",
    "    image=soil_moisture,\n",
    "    description=f'BangladeshSoilMoisture{soil_moisture_date.year}{soil_moisture_date.month:02}{soil_moisture_date.day:02}',\n",
    "    folder='BangladeshSoilMoisture',\n",
    "    fileNamePrefix=f'BangladeshSoilMoisture{soil_moisture_date.year}{soil_moisture_date.month:02}{soil_moisture_date.day:02}',\n",
    "    region=bbox_ee,\n",
    "    crs='EPSG:4326',\n",
    "    # scale=250,  # Adjust the scale as needed\n",
    "    crsTransform=[0.002245788210298804, 0.0, 88.08430518433968, 0.0, -0.002245788210298804, 26.44864775268901],\n",
    "    maxPixels=1e13 \n",
    ")\n",
    "export_task.start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'state': 'READY',\n",
       " 'description': 'BangladeshSoilMoisture20010707',\n",
       " 'priority': 100,\n",
       " 'creation_timestamp_ms': 1720008450010,\n",
       " 'update_timestamp_ms': 1720008450010,\n",
       " 'start_timestamp_ms': 0,\n",
       " 'task_type': 'EXPORT_IMAGE',\n",
       " 'id': '3PIBKIK3GO5FEQ67SARMUKJ3',\n",
       " 'name': 'projects/ee-varunkhandekar/operations/3PIBKIK3GO5FEQ67SARMUKJ3'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "export_task.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shape Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_country_outline(shapefile_path: str):\n",
    "    with open(shapefile_path, 'r') as file:\n",
    "        geojson_data = json.load(file)\n",
    "\n",
    "    # Extract polygon coordinates\n",
    "    polygon_coordinates = []\n",
    "\n",
    "    for feature in geojson_data['features']:\n",
    "        if feature['geometry']['type'] == 'Polygon':\n",
    "            polygon_coordinates.append(feature['geometry']['coordinates'])\n",
    "        elif feature['geometry']['type'] == 'MultiPolygon':\n",
    "            for polygon in feature['geometry']['coordinates']:\n",
    "                polygon_coordinates.append(polygon)\n",
    "    \n",
    "    return Polygon(polygon_coordinates[0][0])\n",
    "\n",
    "bangladesh_shape = generate_country_outline('bangladesh-outline_68.geojson')\n",
    "bangladesh_bounding_box = box(*bangladesh_shape.bounds)\n",
    "bangladesh_bounding_box_ee = ee.Geometry.BBox(*bangladesh_shape.bounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Soil Moisture Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_soil_moisture(soil_moisture_date, output_path: str, shape: Polygon):\n",
    "    # Set up Copernicus\n",
    "    c = cdsapi.Client()\n",
    "\n",
    "    zip_path = f'data/soil_moisture_bangladesh_{soil_moisture_date.year}{soil_moisture_date.month:02}{soil_moisture_date.day:02}.zip'\n",
    "    # with contextlib.redirect_stdout(None):\n",
    "    c.retrieve(\n",
    "    'satellite-soil-moisture',\n",
    "    {\n",
    "        'format': 'zip',\n",
    "        'year': f'{soil_moisture_date.year}',\n",
    "        'day': f'{soil_moisture_date.day:02}',\n",
    "        'variable': 'surface_soil_moisture',\n",
    "        'type_of_sensor': 'active',\n",
    "        'time_aggregation': 'day_average',\n",
    "        'month': f'{soil_moisture_date.month:02}',\n",
    "        'type_of_record': 'cdr', #may need to change to icdr when considering going 'live'\n",
    "        'version': 'v202212',\n",
    "    },\n",
    "    zip_path\n",
    "    )\n",
    "\n",
    "    new_path = os.path.join(output_path, f'soil_moisture_bangladesh_{soil_moisture_date.year}{soil_moisture_date.month:02}{soil_moisture_date.day:02}.nc')\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        file_list = zip_ref.namelist()\n",
    "        zip_ref.extract(file_list[0], path=output_path)       \n",
    "        os.rename(os.path.join(output_path, file_list[0]), new_path)\n",
    "        print(f'File extracted and renamed to: {new_path}')\n",
    "    os.remove(zip_path)\n",
    "\n",
    "    # Clip the data and save down\n",
    "    geojson = [mapping(shape)] # Use GeoJSON of the shape\n",
    "    with xr.open_dataset(new_path, engine=\"netcdf4\") as data:\n",
    "        data = data['sm']\n",
    "        data.rio.set_spatial_dims('lon', 'lat', inplace=True)\n",
    "        data.rio.write_crs(\"EPSG:4326\", inplace=True)\n",
    "        \n",
    "        data_clipped = data.rio.clip(geojson, all_touched=True)\n",
    "    data_clipped.to_netcdf(new_path, mode='w')\n",
    "\n",
    "    return new_path\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Your browser has been opened to visit:\n",
      "\n",
      "    https://accounts.google.com/o/oauth2/auth?client_id=629975352601-slkn7n0p99o6udme02pkj26fa9vvnb28.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A8080%2F&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&access_type=offline&response_type=code\n",
      "\n",
      "Authentication successful.\n"
     ]
    }
   ],
   "source": [
    "def authenticate(config_file_path: str):\n",
    "    \"\"\"Authenticate using PyDrive and return a GoogleDrive object.\"\"\"\n",
    "    with open(config_file_path) as config_file:\n",
    "        config = json.load(config_file)\n",
    "    google_drive_credentials_path = config['google_drive_credentials']\n",
    "    google_drive_oauth_path = config['google_drive_oauth']\n",
    "\n",
    "    gauth = GoogleAuth()\n",
    "    gauth.LoadClientConfigFile(google_drive_credentials_path)\n",
    "\n",
    "    # Try to load saved credentials\n",
    "    gauth.LoadCredentialsFile(google_drive_oauth_path)\n",
    "    if gauth.credentials is None:\n",
    "        try:\n",
    "            gauth.LocalWebserverAuth()  # Creates local webserver and auto handles authentication\n",
    "        except:\n",
    "            gauth.CommandLineAuth()  # Use this if LocalWebserverAuth fails\n",
    "        gauth.SaveCredentialsFile(google_drive_oauth_path)\n",
    "    elif gauth.access_token_expired:\n",
    "        gauth.Refresh()\n",
    "        gauth.SaveCredentialsFile(google_drive_oauth_path)\n",
    "    else:\n",
    "        gauth.Authorize()\n",
    "\n",
    "    return GoogleDrive(gauth)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_to_google_drive(drive: GoogleDrive, file_path: str, config_file_path: str, target_location: str, overwrite=False):\n",
    "    # Get config details\n",
    "    with open(config_file_path) as config_file:\n",
    "        config = json.load(config_file)\n",
    "    folder_id = config[target_location]\n",
    "\n",
    "    # drive = authenticate(config_file_path)\n",
    "    file_name = os.path.basename(file_path)\n",
    "\n",
    "    if overwrite:\n",
    "        # Search for the file with the same name in the target folder; delete if it exists\n",
    "        file_list = drive.ListFile({'q': f\"title = '{file_name}' and '{folder_id}' in parents and trashed=false\"}).GetList()\n",
    "        for file in file_list:\n",
    "            file.Delete()\n",
    "\n",
    "    file = drive.CreateFile({'title': file_name, 'parents': [{'id': folder_id}]})\n",
    "    file.SetContentFile(file_path)\n",
    "    file.Upload()\n",
    "    file = None\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rainfall Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_timestamps(date: pd.Timestamp, days_before: int, days_after: int):\n",
    "    start_time = date - pd.Timedelta(days=days_before)\n",
    "    end_time = date + pd.Timedelta(days=days_after)\n",
    "\n",
    "    timestamps = pd.date_range(start=start_time, end=end_time, freq='3h')\n",
    "    # don't include final forecast as it takes us into day t+days_after+1\n",
    "    return timestamps.tolist()[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello from the Earth Engine servers!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GoogleDriveFile({'kind': 'drive#file', 'userPermission': {'id': 'me', 'type': 'user', 'role': 'reader', 'kind': 'drive#permission', 'selfLink': 'https://www.googleapis.com/drive/v2/files/1RPdc1H40PgGrySKGiS7NEgjUGsa87KQQ/permissions/me', 'etag': '\"z1iyNVncsY5NbIAaVMiZUu299K8\"', 'pendingOwner': False}, 'fileExtension': 'nc', 'md5Checksum': '9a154d72b00b9effa8419f07238fcf0e', 'selfLink': 'https://www.googleapis.com/drive/v2/files/1RPdc1H40PgGrySKGiS7NEgjUGsa87KQQ', 'ownerNames': ['gloh2o.org'], 'lastModifyingUserName': 'gloh2o.org', 'editable': False, 'writersCanShare': True, 'downloadUrl': 'https://www.googleapis.com/drive/v2/files/1RPdc1H40PgGrySKGiS7NEgjUGsa87KQQ?alt=media&source=downloadUrl', 'mimeType': 'application/x-netcdf', 'parents': [{'selfLink': 'https://www.googleapis.com/drive/v2/files/1RPdc1H40PgGrySKGiS7NEgjUGsa87KQQ/parents/1DVR90Ud1C444bTOPeENX-3I7tgqPLnao', 'id': '1DVR90Ud1C444bTOPeENX-3I7tgqPLnao', 'isRoot': False, 'kind': 'drive#parentReference', 'parentLink': 'https://www.googleapis.com/drive/v2/files/1DVR90Ud1C444bTOPeENX-3I7tgqPLnao'}], 'appDataContents': False, 'iconLink': 'https://drive-thirdparty.googleusercontent.com/16/type/application/x-netcdf', 'shared': True, 'lastModifyingUser': {'displayName': 'gloh2o.org', 'kind': 'drive#user', 'isAuthenticatedUser': False, 'permissionId': '04582572561001553310', 'emailAddress': 'gloh2o.org@gmail.com', 'picture': {'url': 'https://lh3.googleusercontent.com/a-/ALV-UjVl3b9ccNf6GHSdlCykzatj0F9na8qwoLQ0oOyHAl7rr2GLhQ=s64'}}, 'owners': [{'displayName': 'gloh2o.org', 'kind': 'drive#user', 'isAuthenticatedUser': False, 'permissionId': '04582572561001553310', 'emailAddress': 'gloh2o.org@gmail.com', 'picture': {'url': 'https://lh3.googleusercontent.com/a-/ALV-UjVl3b9ccNf6GHSdlCykzatj0F9na8qwoLQ0oOyHAl7rr2GLhQ=s64'}}], 'headRevisionId': '0B2dnSpICnSIVSEs0bWFiYXV4S0l6NjE0dWtjWDdIWURieXlzPQ', 'copyable': True, 'etag': '\"MTY0MDYyNDI2MTE4OQ\"', 'alternateLink': 'https://drive.google.com/file/d/1RPdc1H40PgGrySKGiS7NEgjUGsa87KQQ/view?usp=drivesdk', 'embedLink': 'https://drive.google.com/file/d/1RPdc1H40PgGrySKGiS7NEgjUGsa87KQQ/preview?usp=drivesdk', 'webContentLink': 'https://drive.google.com/uc?id=1RPdc1H40PgGrySKGiS7NEgjUGsa87KQQ&export=download', 'fileSize': '2463376', 'copyRequiresWriterPermission': False, 'spaces': ['drive'], 'id': '1RPdc1H40PgGrySKGiS7NEgjUGsa87KQQ', 'title': '2007174.06.nc', 'description': '2007174.06.nc', 'labels': {'viewed': False, 'restricted': False, 'starred': False, 'hidden': False, 'trashed': False}, 'explicitlyTrashed': False, 'createdDate': '2021-12-27T19:22:46.511Z', 'modifiedDate': '2021-12-27T16:57:41.189Z', 'markedViewedByMeDate': '1970-01-01T00:00:00.000Z', 'quotaBytesUsed': '2463376', 'version': '1', 'originalFilename': '2007174.06.nc', 'capabilities': {'canEdit': False, 'canCopy': True}})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_date_to_MSWEP_file_name(timestamp: pd.Timestamp):\n",
    "    return f\"{timestamp.year}{timestamp.dayofyear}.{timestamp.hour:02}.nc\"\n",
    "\n",
    "def generate_files_of_interest(drive: GoogleDrive, timestamps: list, config: str, folder: str):\n",
    "    with open(\"config.json\") as config_file:\n",
    "        config = json.load(config_file)\n",
    "    folder_id = config[folder]\n",
    "\n",
    "    file_names = [convert_date_to_MSWEP_file_name(timestamp) for timestamp in timestamps]\n",
    "\n",
    "    files = []\n",
    "    for file_name in file_names:\n",
    "        query = f\"title='{file_name}' and '{folder_id}' in parents\"\n",
    "        file_list = drive.ListFile({'q': query}).GetList()\n",
    "        files.extend(file_list)\n",
    "    return files\n",
    "\n",
    "\n",
    "ee.Authenticate()\n",
    "with open('config.json') as config_file:\n",
    "    config = json.load(config_file)\n",
    "earth_engine_project_name = config['earth_engine_project']\n",
    "ee.Initialize(project=earth_engine_project_name)\n",
    "print(ee.String('Hello from the Earth Engine servers!').getInfo())\n",
    "\n",
    "\n",
    "perm_water_threshold = 50 # x% of the time the area is covered in water\n",
    "\n",
    "\n",
    "drive = authenticate('config.json')\n",
    "query = f\"title='2007174.06.nc' and '1DVR90Ud1C444bTOPeENX-3I7tgqPLnao' in parents\"\n",
    "file_list = drive.ListFile({'q': query}).GetList()\n",
    "file_list[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2007-06-23 06:00:00')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timestamp_str = \"2007174.06\"\n",
    "\n",
    "# Extract the components from the string\n",
    "year = int(timestamp_str[:4])\n",
    "day_of_year = int(timestamp_str[4:7])\n",
    "hour = int(float(timestamp_str[7:]) * 100)\n",
    "\n",
    "# Construct the timestamp\n",
    "timestamp = pd.Timestamp(year=year, month=1, day=1) + pd.to_timedelta(f'{day_of_year - 1} days') + pd.to_timedelta(f'{hour} hours')\n",
    "timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_output_path = \"./data/temp/\"\n",
    "new_file = drive.CreateFile({'id': file_list[0]['id']})\n",
    "new_path = os.path.join(temp_output_path, f\"{timestamp.year}{timestamp.dayofyear}.{timestamp.hour:02}.nc\")\n",
    "temp_path = os.path.join(temp_output_path, f\"{timestamp.year}{timestamp.dayofyear}.{timestamp.hour:02}_tmp.nc\")\n",
    "new_file.GetContentFile(new_path)\n",
    "\n",
    "# Clip the data and save down\n",
    "geojson = [mapping(bangladesh_bounding_box)] # Use GeoJSON of the shape\n",
    "with xr.open_dataset(new_path, engine=\"netcdf4\") as data:\n",
    "    data = data['precipitation']\n",
    "    data.rio.set_spatial_dims('lon', 'lat', inplace=True)\n",
    "    data.rio.write_crs(\"EPSG:4326\", inplace=True)\n",
    "    \n",
    "    data_clipped = data.rio.clip(geojson, all_touched=True)\n",
    "    data_clipped = ((data_clipped*1000)//1).astype(np.uint16)\n",
    "data_clipped.to_netcdf(temp_path, mode='w')\n",
    "os.remove(new_path)\n",
    "os.rename(temp_path, new_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><svg style=\"position: absolute; width: 0; height: 0; overflow: hidden\">\n",
       "<defs>\n",
       "<symbol id=\"icon-database\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M16 0c-8.837 0-16 2.239-16 5v4c0 2.761 7.163 5 16 5s16-2.239 16-5v-4c0-2.761-7.163-5-16-5z\"></path>\n",
       "<path d=\"M16 17c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "<path d=\"M16 26c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "</symbol>\n",
       "<symbol id=\"icon-file-text2\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M28.681 7.159c-0.694-0.947-1.662-2.053-2.724-3.116s-2.169-2.030-3.116-2.724c-1.612-1.182-2.393-1.319-2.841-1.319h-15.5c-1.378 0-2.5 1.121-2.5 2.5v27c0 1.378 1.122 2.5 2.5 2.5h23c1.378 0 2.5-1.122 2.5-2.5v-19.5c0-0.448-0.137-1.23-1.319-2.841zM24.543 5.457c0.959 0.959 1.712 1.825 2.268 2.543h-4.811v-4.811c0.718 0.556 1.584 1.309 2.543 2.268zM28 29.5c0 0.271-0.229 0.5-0.5 0.5h-23c-0.271 0-0.5-0.229-0.5-0.5v-27c0-0.271 0.229-0.5 0.5-0.5 0 0 15.499-0 15.5 0v7c0 0.552 0.448 1 1 1h7v19.5z\"></path>\n",
       "<path d=\"M23 26h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 22h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 18h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "</symbol>\n",
       "</defs>\n",
       "</svg>\n",
       "<style>/* CSS stylesheet for displaying xarray objects in jupyterlab.\n",
       " *\n",
       " */\n",
       "\n",
       ":root {\n",
       "  --xr-font-color0: var(--jp-content-font-color0, rgba(0, 0, 0, 1));\n",
       "  --xr-font-color2: var(--jp-content-font-color2, rgba(0, 0, 0, 0.54));\n",
       "  --xr-font-color3: var(--jp-content-font-color3, rgba(0, 0, 0, 0.38));\n",
       "  --xr-border-color: var(--jp-border-color2, #e0e0e0);\n",
       "  --xr-disabled-color: var(--jp-layout-color3, #bdbdbd);\n",
       "  --xr-background-color: var(--jp-layout-color0, white);\n",
       "  --xr-background-color-row-even: var(--jp-layout-color1, white);\n",
       "  --xr-background-color-row-odd: var(--jp-layout-color2, #eeeeee);\n",
       "}\n",
       "\n",
       "html[theme=dark],\n",
       "body[data-theme=dark],\n",
       "body.vscode-dark {\n",
       "  --xr-font-color0: rgba(255, 255, 255, 1);\n",
       "  --xr-font-color2: rgba(255, 255, 255, 0.54);\n",
       "  --xr-font-color3: rgba(255, 255, 255, 0.38);\n",
       "  --xr-border-color: #1F1F1F;\n",
       "  --xr-disabled-color: #515151;\n",
       "  --xr-background-color: #111111;\n",
       "  --xr-background-color-row-even: #111111;\n",
       "  --xr-background-color-row-odd: #313131;\n",
       "}\n",
       "\n",
       ".xr-wrap {\n",
       "  display: block !important;\n",
       "  min-width: 300px;\n",
       "  max-width: 700px;\n",
       "}\n",
       "\n",
       ".xr-text-repr-fallback {\n",
       "  /* fallback to plain text repr when CSS is not injected (untrusted notebook) */\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-header {\n",
       "  padding-top: 6px;\n",
       "  padding-bottom: 6px;\n",
       "  margin-bottom: 4px;\n",
       "  border-bottom: solid 1px var(--xr-border-color);\n",
       "}\n",
       "\n",
       ".xr-header > div,\n",
       ".xr-header > ul {\n",
       "  display: inline;\n",
       "  margin-top: 0;\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-obj-type,\n",
       ".xr-array-name {\n",
       "  margin-left: 2px;\n",
       "  margin-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-obj-type {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-sections {\n",
       "  padding-left: 0 !important;\n",
       "  display: grid;\n",
       "  grid-template-columns: 150px auto auto 1fr 20px 20px;\n",
       "}\n",
       "\n",
       ".xr-section-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-section-item input {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-item input + label {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label {\n",
       "  cursor: pointer;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label:hover {\n",
       "  color: var(--xr-font-color0);\n",
       "}\n",
       "\n",
       ".xr-section-summary {\n",
       "  grid-column: 1;\n",
       "  color: var(--xr-font-color2);\n",
       "  font-weight: 500;\n",
       "}\n",
       "\n",
       ".xr-section-summary > span {\n",
       "  display: inline-block;\n",
       "  padding-left: 0.5em;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in + label:before {\n",
       "  display: inline-block;\n",
       "  content: '►';\n",
       "  font-size: 11px;\n",
       "  width: 15px;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label:before {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label:before {\n",
       "  content: '▼';\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label > span {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-summary,\n",
       ".xr-section-inline-details {\n",
       "  padding-top: 4px;\n",
       "  padding-bottom: 4px;\n",
       "}\n",
       "\n",
       ".xr-section-inline-details {\n",
       "  grid-column: 2 / -1;\n",
       "}\n",
       "\n",
       ".xr-section-details {\n",
       "  display: none;\n",
       "  grid-column: 1 / -1;\n",
       "  margin-bottom: 5px;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked ~ .xr-section-details {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-array-wrap {\n",
       "  grid-column: 1 / -1;\n",
       "  display: grid;\n",
       "  grid-template-columns: 20px auto;\n",
       "}\n",
       "\n",
       ".xr-array-wrap > label {\n",
       "  grid-column: 1;\n",
       "  vertical-align: top;\n",
       "}\n",
       "\n",
       ".xr-preview {\n",
       "  color: var(--xr-font-color3);\n",
       "}\n",
       "\n",
       ".xr-array-preview,\n",
       ".xr-array-data {\n",
       "  padding: 0 5px !important;\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-array-data,\n",
       ".xr-array-in:checked ~ .xr-array-preview {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-array-in:checked ~ .xr-array-data,\n",
       ".xr-array-preview {\n",
       "  display: inline-block;\n",
       "}\n",
       "\n",
       ".xr-dim-list {\n",
       "  display: inline-block !important;\n",
       "  list-style: none;\n",
       "  padding: 0 !important;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list li {\n",
       "  display: inline-block;\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list:before {\n",
       "  content: '(';\n",
       "}\n",
       "\n",
       ".xr-dim-list:after {\n",
       "  content: ')';\n",
       "}\n",
       "\n",
       ".xr-dim-list li:not(:last-child):after {\n",
       "  content: ',';\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-has-index {\n",
       "  font-weight: bold;\n",
       "}\n",
       "\n",
       ".xr-var-list,\n",
       ".xr-var-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-var-item > div,\n",
       ".xr-var-item label,\n",
       ".xr-var-item > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-even);\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-var-item > .xr-var-name:hover span {\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-var-list > li:nth-child(odd) > div,\n",
       ".xr-var-list > li:nth-child(odd) > label,\n",
       ".xr-var-list > li:nth-child(odd) > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-odd);\n",
       "}\n",
       "\n",
       ".xr-var-name {\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-var-dims {\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-var-dtype {\n",
       "  grid-column: 3;\n",
       "  text-align: right;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-var-preview {\n",
       "  grid-column: 4;\n",
       "}\n",
       "\n",
       ".xr-index-preview {\n",
       "  grid-column: 2 / 5;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-var-name,\n",
       ".xr-var-dims,\n",
       ".xr-var-dtype,\n",
       ".xr-preview,\n",
       ".xr-attrs dt {\n",
       "  white-space: nowrap;\n",
       "  overflow: hidden;\n",
       "  text-overflow: ellipsis;\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-var-name:hover,\n",
       ".xr-var-dims:hover,\n",
       ".xr-var-dtype:hover,\n",
       ".xr-attrs dt:hover {\n",
       "  overflow: visible;\n",
       "  width: auto;\n",
       "  z-index: 1;\n",
       "}\n",
       "\n",
       ".xr-var-attrs,\n",
       ".xr-var-data,\n",
       ".xr-index-data {\n",
       "  display: none;\n",
       "  background-color: var(--xr-background-color) !important;\n",
       "  padding-bottom: 5px !important;\n",
       "}\n",
       "\n",
       ".xr-var-attrs-in:checked ~ .xr-var-attrs,\n",
       ".xr-var-data-in:checked ~ .xr-var-data,\n",
       ".xr-index-data-in:checked ~ .xr-index-data {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       ".xr-var-data > table {\n",
       "  float: right;\n",
       "}\n",
       "\n",
       ".xr-var-name span,\n",
       ".xr-var-data,\n",
       ".xr-index-name div,\n",
       ".xr-index-data,\n",
       ".xr-attrs {\n",
       "  padding-left: 25px !important;\n",
       "}\n",
       "\n",
       ".xr-attrs,\n",
       ".xr-var-attrs,\n",
       ".xr-var-data,\n",
       ".xr-index-data {\n",
       "  grid-column: 1 / -1;\n",
       "}\n",
       "\n",
       "dl.xr-attrs {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  display: grid;\n",
       "  grid-template-columns: 125px auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt,\n",
       ".xr-attrs dd {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  float: left;\n",
       "  padding-right: 10px;\n",
       "  width: auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt {\n",
       "  font-weight: normal;\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-attrs dt:hover span {\n",
       "  display: inline-block;\n",
       "  background: var(--xr-background-color);\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-attrs dd {\n",
       "  grid-column: 2;\n",
       "  white-space: pre-wrap;\n",
       "  word-break: break-all;\n",
       "}\n",
       "\n",
       ".xr-icon-database,\n",
       ".xr-icon-file-text2,\n",
       ".xr-no-icon {\n",
       "  display: inline-block;\n",
       "  vertical-align: middle;\n",
       "  width: 1em;\n",
       "  height: 1.5em !important;\n",
       "  stroke-width: 0;\n",
       "  stroke: currentColor;\n",
       "  fill: currentColor;\n",
       "}\n",
       "</style><pre class='xr-text-repr-fallback'>&lt;xarray.DataArray &#x27;precipitation&#x27; ()&gt; Size: 2B\n",
       "array(0, dtype=uint16)\n",
       "Coordinates:\n",
       "    spatial_ref  int64 8B 0</pre><div class='xr-wrap' style='display:none'><div class='xr-header'><div class='xr-obj-type'>xarray.DataArray</div><div class='xr-array-name'>'precipitation'</div></div><ul class='xr-sections'><li class='xr-section-item'><div class='xr-array-wrap'><input id='section-471b6308-4dce-471e-8b51-ded3d8b28ef8' class='xr-array-in' type='checkbox' checked><label for='section-471b6308-4dce-471e-8b51-ded3d8b28ef8' title='Show/hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-array-preview xr-preview'><span>0</span></div><div class='xr-array-data'><pre>array(0, dtype=uint16)</pre></div></div></li><li class='xr-section-item'><input id='section-3978ec30-ec64-45f6-b493-12d13f0b5100' class='xr-section-summary-in' type='checkbox'  checked><label for='section-3978ec30-ec64-45f6-b493-12d13f0b5100' class='xr-section-summary' >Coordinates: <span>(1)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span>spatial_ref</span></div><div class='xr-var-dims'>()</div><div class='xr-var-dtype'>int64</div><div class='xr-var-preview xr-preview'>0</div><input id='attrs-96bc0ffd-6a4c-4612-a5d7-09b7506b3dbd' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-96bc0ffd-6a4c-4612-a5d7-09b7506b3dbd' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-5a7f93b1-7770-4c1d-abbc-626ef4577f0c' class='xr-var-data-in' type='checkbox'><label for='data-5a7f93b1-7770-4c1d-abbc-626ef4577f0c' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>crs_wkt :</span></dt><dd>GEOGCS[&quot;WGS 84&quot;,DATUM[&quot;WGS_1984&quot;,SPHEROID[&quot;WGS 84&quot;,6378137,298.257223563,AUTHORITY[&quot;EPSG&quot;,&quot;7030&quot;]],AUTHORITY[&quot;EPSG&quot;,&quot;6326&quot;]],PRIMEM[&quot;Greenwich&quot;,0,AUTHORITY[&quot;EPSG&quot;,&quot;8901&quot;]],UNIT[&quot;degree&quot;,0.0174532925199433,AUTHORITY[&quot;EPSG&quot;,&quot;9122&quot;]],AXIS[&quot;Latitude&quot;,NORTH],AXIS[&quot;Longitude&quot;,EAST],AUTHORITY[&quot;EPSG&quot;,&quot;4326&quot;]]</dd><dt><span>semi_major_axis :</span></dt><dd>6378137.0</dd><dt><span>semi_minor_axis :</span></dt><dd>6356752.314245179</dd><dt><span>inverse_flattening :</span></dt><dd>298.257223563</dd><dt><span>reference_ellipsoid_name :</span></dt><dd>WGS 84</dd><dt><span>longitude_of_prime_meridian :</span></dt><dd>0.0</dd><dt><span>prime_meridian_name :</span></dt><dd>Greenwich</dd><dt><span>geographic_crs_name :</span></dt><dd>WGS 84</dd><dt><span>horizontal_datum_name :</span></dt><dd>World Geodetic System 1984</dd><dt><span>grid_mapping_name :</span></dt><dd>latitude_longitude</dd><dt><span>spatial_ref :</span></dt><dd>GEOGCS[&quot;WGS 84&quot;,DATUM[&quot;WGS_1984&quot;,SPHEROID[&quot;WGS 84&quot;,6378137,298.257223563,AUTHORITY[&quot;EPSG&quot;,&quot;7030&quot;]],AUTHORITY[&quot;EPSG&quot;,&quot;6326&quot;]],PRIMEM[&quot;Greenwich&quot;,0,AUTHORITY[&quot;EPSG&quot;,&quot;8901&quot;]],UNIT[&quot;degree&quot;,0.0174532925199433,AUTHORITY[&quot;EPSG&quot;,&quot;9122&quot;]],AXIS[&quot;Latitude&quot;,NORTH],AXIS[&quot;Longitude&quot;,EAST],AUTHORITY[&quot;EPSG&quot;,&quot;4326&quot;]]</dd><dt><span>GeoTransform :</span></dt><dd>88.00000298541525 0.10000013268512228 0.0 26.50000078924771 0.0 -0.10000005261651401</dd></dl></div><div class='xr-var-data'><pre>array(0)</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-0cab8f52-e86b-4244-a1cb-ad966261e2c1' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-0cab8f52-e86b-4244-a1cb-ad966261e2c1' class='xr-section-summary'  title='Expand/collapse section'>Indexes: <span>(0)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'></ul></div></li><li class='xr-section-item'><input id='section-09a5e21b-8dd1-4912-a382-d1f976574a6c' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-09a5e21b-8dd1-4912-a382-d1f976574a6c' class='xr-section-summary'  title='Expand/collapse section'>Attributes: <span>(0)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><dl class='xr-attrs'></dl></div></li></ul></div></div>"
      ],
      "text/plain": [
       "<xarray.DataArray 'precipitation' ()> Size: 2B\n",
       "array(0, dtype=uint16)\n",
       "Coordinates:\n",
       "    spatial_ref  int64 8B 0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data_clipped.min()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Now convert to .tif using GDAL\n",
    "lowres_tif_file = os.path.join(temp_output_path, f\"{timestamp.year}{timestamp.dayofyear}.{timestamp.hour:02}_lowres.tif\")\n",
    "precipitation_data = gdal.Open(f'NETCDF:\"{new_path}\":precipitation')\n",
    "driver = gdal.GetDriverByName('GTiff')\n",
    "\n",
    "output_dataset = driver.CreateCopy(lowres_tif_file, precipitation_data, 0)\n",
    "output_dataset.SetProjection('EPSG:4326')\n",
    "\n",
    "precipitation_data = None\n",
    "output_dataset = None\n",
    "\n",
    "# os.remove(new_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reproject_and_upsample(input_file: str, output_file: str, config_file: str):\n",
    "    with rasterio.open(input_file) as src:\n",
    "        src_transform = src.transform\n",
    "        data = src.read()\n",
    "        src_crs = src.crs\n",
    "        with open(\"config.json\") as config_file:\n",
    "            config = json.load(config_file)\n",
    "        master_file = config['rainfall_reprojection_master']\n",
    "        # Open target file to get target dimensions and transform\n",
    "        with rasterio.open(master_file) as target:\n",
    "            target_transform = target.transform\n",
    "            target_width = target.width\n",
    "            target_height = target.height\n",
    "            target_crs = target.crs\n",
    "            \n",
    "            # Update metadata for the new file, including LZW compression\n",
    "            kwargs = src.meta\n",
    "            kwargs.update({\n",
    "                'height': target_height,\n",
    "                'width': target_width,\n",
    "                'transform': target_transform,\n",
    "                'crs': target_crs,\n",
    "                'compress': 'lzw'\n",
    "            })\n",
    "        \n",
    "        # Write to output\n",
    "        with rasterio.open(output_file, 'w', **kwargs) as dst:\n",
    "            for i, band in enumerate(data, 1): # Go through bands\n",
    "                dest = np.zeros((target_height, target_width), dtype=np.float32)\n",
    "                \n",
    "                reproject(\n",
    "                    source=band,\n",
    "                    destination=dest,\n",
    "                    src_transform=src_transform,\n",
    "                    src_crs=src_crs,\n",
    "                    dst_transform=target_transform,\n",
    "                    dst_crs=target_crs,\n",
    "                    resampling=Resampling.bilinear\n",
    "                )\n",
    "\n",
    "                dst.write(dest, indexes=i)\n",
    "\n",
    "    return output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59\n",
      "I;16\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "# Do reprojection, upsampling and compression before saving down\n",
    "\n",
    "output_tif_file = os.path.join(temp_output_path, f\"{timestamp.year}{timestamp.dayofyear}.{timestamp.hour:02}_test.tif\")\n",
    "\n",
    "\n",
    "lowres_image = Image.open(lowres_tif_file)\n",
    "print(lowres_image.height)\n",
    "with open(\"config.json\") as config_file:\n",
    "    config = json.load(config_file)\n",
    "master_file = config['rainfall_reprojection_master']\n",
    "target_image = Image.open(master_file)\n",
    "target_width = target_image.width\n",
    "target_height = target_image.height\n",
    "print(lowres_image.mode)\n",
    "upsampled_image = lowres_image.resize((target_width, target_height))\n",
    "upsampled_image.save(output_tif_file, compression='tiff_deflate')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'driver': 'GTiff', 'dtype': 'uint16', 'nodata': 65535.0, 'width': 47, 'height': 59, 'count': 1, 'crs': CRS.from_epsg(4326), 'transform': Affine(0.10000013268512228, 0.0, 88.00000298541525,\n",
      "       0.0, -0.10000005261651401, 26.50000078924771)}\n",
      "(1, 59, 47)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with rasterio.open(lowres_tif_file) as src:\n",
    "    print(src.meta)\n",
    "    data = src.read()\n",
    "    print(data.shape)\n",
    "    # with rasterio.open(master_file) as target:\n",
    "    #     target_transform = target.transform\n",
    "    #     target_width = target.width\n",
    "    #     target_height = target.height\n",
    "    #     target_crs = target.crs\n",
    "        \n",
    "    #     kwargs = src.meta\n",
    "    #     kwargs.update({\n",
    "    #         'height': target_height,\n",
    "    #         'width': target_width,\n",
    "    #         'transform': target_transform,\n",
    "    #         'crs': target_crs,\n",
    "    #     })\n",
    "    # # Write to output\n",
    "    # with rasterio.open(os.path.join(temp_output_path, f\"{timestamp.year}{timestamp.dayofyear}.{timestamp.hour:02}_test_withmeta.tif\"), 'w', **kwargs) as dst:\n",
    "    #     dst.write(data[0], indexes=1)\n",
    "\n",
    "# reproject_and_upsample(lowres_tif_file, output_tif_file, config_file) #2044x2573\n",
    "# os.remove(lowres_tif_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'driver': 'GTiff', 'dtype': 'float32', 'nodata': None, 'width': 2044, 'height': 2573, 'count': 1, 'crs': None, 'transform': Affine(1.0, 0.0, 0.0,\n",
      "       0.0, 1.0, 0.0)}\n",
      "(1, 2573, 2044)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vkhandekar/project_flood/.venv/lib/python3.10/site-packages/rasterio/__init__.py:304: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  dataset = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "def remove_metadata(image_path: str):\n",
    "    image = Image.open(image_path)\n",
    "    data = list(image.getdata())\n",
    "    image_without_metadata = Image.new(image.mode, image.size)\n",
    "    image_without_metadata.putdata(data)\n",
    "    image_without_metadata.save(image_path)\n",
    "\n",
    "\n",
    "remove_metadata(os.path.join(temp_output_path, \"2007181.21.tif\"))\n",
    "\n",
    "with rasterio.open(os.path.join(temp_output_path, \"2007181.21.tif\")) as src:\n",
    "    print(src.meta)\n",
    "    data = src.read()\n",
    "    print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_and_crop_rainfall_data(drive: GoogleDrive, original_file, timestamp: pd.Timestamp, temp_output_path: str, shape: Polygon, config_file: str):\n",
    "\n",
    "    new_file = drive.CreateFile({'id': original_file['id']})\n",
    "    new_path = os.path.join(temp_output_path, f\"{timestamp.year}{timestamp.dayofyear}.{timestamp.hour:02}.nc\")\n",
    "    temp_path = os.path.join(temp_output_path, f\"{timestamp.year}{timestamp.dayofyear}.{timestamp.hour:02}_tmp.nc\")\n",
    "    new_file.GetContentFile(new_path)\n",
    "\n",
    "    # Clip the data and save down\n",
    "    geojson = [mapping(shape)] # Use GeoJSON of the shape\n",
    "    with xr.open_dataset(new_path, engine=\"netcdf4\") as data:\n",
    "        data = data['precipitation']\n",
    "        data.rio.set_spatial_dims('lon', 'lat', inplace=True)\n",
    "        data.rio.write_crs(\"EPSG:4326\", inplace=True)\n",
    "        \n",
    "        data_clipped = data.rio.clip(geojson, all_touched=True)\n",
    "    data_clipped.to_netcdf(temp_path, mode='w')\n",
    "    os.remove(new_path)\n",
    "    os.rename(temp_path, new_path)\n",
    "\n",
    "    # Now convert to .tif using GDAL\n",
    "    lowres_tif_file = os.path.join(temp_output_path, f\"{timestamp.year}{timestamp.dayofyear}.{timestamp.hour:02}_lowres.tif\")\n",
    "    precipitation_data = gdal.Open(f'NETCDF:\"{new_path}\":precipitation')\n",
    "    driver = gdal.GetDriverByName('GTiff')\n",
    "\n",
    "    output_dataset = driver.CreateCopy(lowres_tif_file, precipitation_data, 0)\n",
    "    output_dataset.SetProjection('EPSG:4326')\n",
    "\n",
    "    precipitation_data = None\n",
    "    output_dataset = None\n",
    "\n",
    "    os.remove(new_path)\n",
    "\n",
    "    # Do reprojection, upsampling and compression before saving down\n",
    "    output_tif_file = os.path.join(temp_output_path, f\"{timestamp.year}{timestamp.dayofyear}.{timestamp.hour:02}.tif\")\n",
    "    reproject_and_upsample(lowres_tif_file, output_tif_file, config_file) #2044x2573\n",
    "    os.remove(lowres_tif_file)\n",
    "\n",
    "    return output_tif_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise and Set Up APIs and Shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello from the Earth Engine servers!\n"
     ]
    }
   ],
   "source": [
    "# Set Up Google Earth Engine\n",
    "ee.Authenticate()\n",
    "with open('config.json') as config_file:\n",
    "    config = json.load(config_file)\n",
    "earth_engine_project_name = config['earth_engine_project']\n",
    "ee.Initialize(project=earth_engine_project_name)\n",
    "print(ee.String('Hello from the Earth Engine servers!').getInfo())\n",
    "\n",
    "gfd = ee.ImageCollection('GLOBAL_FLOOD_DB/MODIS_EVENTS/V1')\n",
    "perm_water = ee.ImageCollection('JRC/GSW1_4/MonthlyRecurrence')\n",
    "perm_water_threshold = 25 # x% of the time the area is covered in water\n",
    "\n",
    "# Set Up Google Drive API\n",
    "drive = authenticate(\"config.json\")\n",
    "\n",
    "# Get Bangladesh outline\n",
    "bangaldesh_coordinates = generate_country_outline('bangladesh-outline_68.geojson')\n",
    "bangladesh_shape_ee = ee.Geometry.MultiPolygon(bangaldesh_coordinates)\n",
    "bangladesh_shape = Polygon(bangaldesh_coordinates[0][0])\n",
    "\n",
    "# Convert the polygon to GeoJSON format\n",
    "# geojson = [mapping(bangladesh_shape)]\n",
    "\n",
    "# Rainfall days before and after\n",
    "rainfall_days_before = 2\n",
    "rainfall_days_after = 1\n",
    "\n",
    "# Max number of threads allowed to run\n",
    "max_workers = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pull Topology Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'state': 'READY',\n",
       " 'description': 'BangladeshTopology',\n",
       " 'priority': 100,\n",
       " 'creation_timestamp_ms': 1719478938633,\n",
       " 'update_timestamp_ms': 1719478938633,\n",
       " 'start_timestamp_ms': 0,\n",
       " 'task_type': 'EXPORT_IMAGE',\n",
       " 'id': '5ACKIZ2XEDOHQN5DR2RGDOW2',\n",
       " 'name': 'projects/ee-varunkhandekar/operations/5ACKIZ2XEDOHQN5DR2RGDOW2'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topology = ee.Image(\"NASA/NASADEM_HGT/001\").select(\"elevation\")\n",
    "\n",
    "# Replace all no data values with 0 (essentially the sea)\n",
    "topology = topology.unmask(0)\n",
    "\n",
    "export_task = ee.batch.Export.image.toDrive(\n",
    "    image=topology,\n",
    "    description='BangladeshTopology',\n",
    "    folder='EarthEngineExports/Topology',\n",
    "    fileNamePrefix='BangladeshTopology',\n",
    "    region=bangladesh_shape_ee.getInfo()['coordinates'],\n",
    "    scale=250,  # Adjust the scale as needed\n",
    "    maxPixels=1e13  # Adjust maxPixels as needed\n",
    ")\n",
    "export_task.start()\n",
    "# export_task.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pulling Requisite Flood Event Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through refined flood events\n",
    "refined_flood_events = generate_flood_events('Bangladesh_Flood_Events.xlsx')\n",
    "\n",
    "for date, events in refined_flood_events.items():\n",
    "    print(\"============\")\n",
    "    print(\"Working on the floods on:\", date)\n",
    "    # Pull and mosaic overlapping flood events, 250m resolution\n",
    "    print(\"----\", \"Generating flood image\", \"----\")\n",
    "    inundation_images = []\n",
    "    for event in events:\n",
    "        flood_image = ee.Image(gfd.filterMetadata('id', 'equals', int(event)).first()).select(['flooded'])\n",
    "        inundation_images.append(flood_image)\n",
    "\n",
    "    # Add permanent water overlay\n",
    "    water_overlay = ee.Image(perm_water.filterMetadata('month', 'equals', date.month).first())\n",
    "    occurrence_band = water_overlay.select('monthly_recurrence')\n",
    "    water_overlay_filtered = occurrence_band.gt(perm_water_threshold)\n",
    "    water_overlay_filtered = water_overlay_filtered.rename('flooded')\n",
    "    inundation_images.append(water_overlay_filtered)\n",
    "\n",
    "    # Export combo image to gdrive\n",
    "    combo = ee.ImageCollection(inundation_images).reduce(ee.Reducer.max())\n",
    "    export_task = ee.batch.Export.image.toDrive(\n",
    "        image=combo,\n",
    "        description=f'BangladeshWater{date.year}{date.month:02}{date.day:02}',\n",
    "        folder='EarthEngineExports/BangladeshFloodImages',\n",
    "        fileNamePrefix=f'BangladeshWater{date.year}{date.month:02}{date.day:02}',\n",
    "        region=bangladesh_shape_ee.getInfo()['coordinates'],\n",
    "        scale=250,  # Adjust the scale as needed\n",
    "        maxPixels=1e13 \n",
    "    )\n",
    "    export_task.start()\n",
    "    export_task.status()\n",
    "    print(\"----\", \"Flood image done\", \"----\", \"\\n\")\n",
    "\n",
    "    # Pull soil moisture data for the day before, store to gdrive\n",
    "    print(\"----\", \"Gathering soil moisture images\", \"----\")\n",
    "    soil_moisture_date = pd.to_datetime(date) - timedelta(days=1)\n",
    "    soil_moisture_file = pull_soil_moisture(soil_moisture_date, 'data/soil_moisture', bangladesh_shape)\n",
    "    send_to_google_drive(drive, soil_moisture_file, 'config.json', 'soil_moisture_folder_id')\n",
    "    print(\"----\", \"Soil moisture image done\", \"----\", \"\\n\")\n",
    "    \n",
    "\n",
    "    # Pull rain data 2 days preceding, 1 day 'forecast'\n",
    "    print(\"----\", \"Gathering rain images\", \"----\")\n",
    "    timestamps = generate_timestamps(date, rainfall_days_before, rainfall_days_after)\n",
    "    files = generate_files_of_interest(drive, timestamps, 'config.json', 'MSWEP_Past_3hr_folder_id')\n",
    "\n",
    "    for i, timestamp in enumerate(timestamps):\n",
    "        print(\"Rain image \", i)\n",
    "        rainfall_file = pull_and_crop_rainfall_data(drive, files[i], timestamp, \"./data/temp/\", bangladesh_shape)\n",
    "        send_to_google_drive(drive, rainfall_file, 'config.json', 'bangladesh_rainfall_folder_id')\n",
    "        \n",
    "    print(\"----\", \"Rain images done\", \"----\")\n",
    "\n",
    "\n",
    "    print(\"============\")    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_flood_event(date, events):\n",
    "    # print(\"============\")\n",
    "    # print(\"Working on the floods on:\", date)\n",
    "    \n",
    "    # Pull and mosaic overlapping flood events, 250m resolution\n",
    "    # print(\"----\", \"Generating flood image\", \"----\")\n",
    "    inundation_images = []\n",
    "    for event in events:\n",
    "        flood_image = ee.Image(gfd.filterMetadata('id', 'equals', int(event)).first()).select(['flooded'])\n",
    "        inundation_images.append(flood_image)\n",
    "\n",
    "    # Add permanent water overlay\n",
    "    water_overlay = ee.Image(perm_water.filterMetadata('month', 'equals', date.month).first())\n",
    "    occurrence_band = water_overlay.select('monthly_recurrence')\n",
    "    water_overlay_filtered = occurrence_band.gt(perm_water_threshold)\n",
    "    water_overlay_filtered = water_overlay_filtered.rename('flooded')\n",
    "    inundation_images.append(water_overlay_filtered)\n",
    "\n",
    "    # Export combo image to gdrive\n",
    "    combo = ee.ImageCollection(inundation_images).reduce(ee.Reducer.max())\n",
    "    export_task = ee.batch.Export.image.toDrive(\n",
    "        image=combo,\n",
    "        description=f'BangladeshWater{date.year}{date.month:02}{date.day:02}',\n",
    "        folder='EarthEngineExports/BangladeshFloodImages',\n",
    "        fileNamePrefix=f'BangladeshWater{date.year}{date.month:02}{date.day:02}',\n",
    "        region=bangladesh_shape_ee.getInfo()['coordinates'],\n",
    "        scale=250,  # Adjust the scale as needed\n",
    "        maxPixels=1e13 \n",
    "    )\n",
    "    export_task.start()\n",
    "    export_task.status()\n",
    "    # print(\"----\", \"Flood image done\", \"----\", \"\\n\")\n",
    "\n",
    "    # Pull soil moisture data for the day before, store to gdrive\n",
    "    # print(\"----\", \"Gathering soil moisture images\", \"----\")\n",
    "    soil_moisture_date = pd.to_datetime(date) - timedelta(days=1)\n",
    "    soil_moisture_file = pull_soil_moisture(soil_moisture_date, 'data/soil_moisture', bangladesh_shape)\n",
    "    send_to_google_drive(drive, soil_moisture_file, 'config.json', 'soil_moisture_folder_id')\n",
    "    # print(\"----\", \"Soil moisture image done\", \"----\", \"\\n\")\n",
    "    \n",
    "    # Pull rain data 2 days preceding, 1 day 'forecast'\n",
    "    # print(\"----\", \"Gathering rain images\", \"----\")\n",
    "    timestamps = generate_timestamps(date, rainfall_days_before, rainfall_days_after)\n",
    "    files = generate_files_of_interest(drive, timestamps, 'config.json', 'MSWEP_Past_3hr_folder_id')\n",
    "\n",
    "    for i, timestamp in enumerate(timestamps):\n",
    "        # print(\"Rain image \", i)\n",
    "        rainfall_file = pull_and_crop_rainfall_data(drive, files[i], timestamp, \"./data/temp/\", bangladesh_shape)\n",
    "        send_to_google_drive(drive, rainfall_file, 'config.json', 'bangladesh_rainfall_folder_id')\n",
    "        \n",
    "    # print(\"----\", \"Rain images done\", \"----\")\n",
    "    # print(\"============\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refined_flood_events = generate_flood_events('Bangladesh_Flood_Events.xlsx')\n",
    "\n",
    "# Submit jobs, thread pool as I/O bound processes i.e. the API requests are the biggest limiting factor\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    futures = [executor.submit(process_flood_event, date, events) for date, events in refined_flood_events.items()]\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        print(future.result())\n",
    "\n",
    "\n",
    "# for future in concurrent.futures.as_completed(futures):\n",
    "#     try:\n",
    "#         future.result()\n",
    "#     except Exception as exc:\n",
    "#         print(f'Generated an exception: {exc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pulling Requisite Non-Flood Event Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the relevant columns for flood events\n",
    "# df_dates = df[['dfo_began_uk', 'dfo_ended_uk']].dropna()\n",
    "\n",
    "def generate_random_non_flood_dates(file_path: str, num_dates: int, safety_window: int):\n",
    "    # Read in data file\n",
    "    if file_path.endswith('.csv'):\n",
    "        df = pd.read_csv(file_path)\n",
    "    elif file_path.endswith('.xlsx') or file_path.endswith('.xls'):\n",
    "        df = pd.read_excel(file_path, engine='openpyxl')\n",
    "    else:\n",
    "        raise ValueError(\"Please provide a CSV or Excel file.\")\n",
    "\n",
    "    # Data file integrity checks\n",
    "    required_columns = ['dfo_began_uk', 'dfo_ended_uk']\n",
    "    assert all(column in df.columns for column in required_columns), f\"Missing columns: {[column for column in required_columns if column not in df.columns]}\"\n",
    "\n",
    "    df['dfo_began_uk'] = pd.to_datetime(df['dfo_began_uk'])\n",
    "    df['dfo_ended_uk'] = pd.to_datetime(df['dfo_ended_uk'])\n",
    "\n",
    "    # Define start and end dates for random date generation\n",
    "    start_date = df['dfo_began_uk'].min()\n",
    "    end_date = df['dfo_ended_uk'].max()\n",
    "\n",
    "    # Define periods to avoid looking at\n",
    "    exclusion_periods = list(zip(df['dfo_began_uk'], df['dfo_ended_uk']))\n",
    "\n",
    "    random_dates = []\n",
    "    while len(random_dates) < num_dates:\n",
    "        random_date = start_date + timedelta(days=np.random.randint(0, (end_date - start_date).days))\n",
    "        # Make sure date is not during any of the periods to be avoided i.e. when there was a flood\n",
    "        if all(not(period_start - timedelta(days=safety_window) <= random_date <= period_end + timedelta(days=safety_window)) \n",
    "               for period_start, period_end in exclusion_periods):\n",
    "            random_dates.append(random_date)\n",
    "    \n",
    "    return random_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_dates_to_generate = 100\n",
    "safety_window = 10\n",
    "random_dates = generate_random_non_flood_dates('Bangladesh_Flood_Events.xlsx', num_dates_to_generate, safety_window)\n",
    "\n",
    "for date in random_dates:\n",
    "    print(\"============\")\n",
    "    print(\"Working on the non-flood event on:\", date)\n",
    "    # Get permanent water\n",
    "    print(\"----\", \"Generating water image\", \"----\")\n",
    "    water_overlay = ee.Image(perm_water.filterMetadata('month', 'equals', date.month).first())\n",
    "    occurrence_band = water_overlay.select('monthly_recurrence')\n",
    "    water_overlay_filtered = occurrence_band.gt(perm_water_threshold) \n",
    "    water_overlay_filtered = water_overlay_filtered.rename('flooded')\n",
    "\n",
    "    export_task = ee.batch.Export.image.toDrive(\n",
    "        image=water_overlay_filtered,\n",
    "        description=f'BangladeshWater{date.year}{date.month:02}{date.day:02}',\n",
    "        folder='EarthEngineExports/BangladeshNonFloodImages',\n",
    "        fileNamePrefix=f'BangladeshWater{date.year}{date.month:02}{date.day:02}',\n",
    "        region=bangladesh_shape_ee.getInfo()['coordinates'],\n",
    "        scale=250,  # Adjust the scale as needed\n",
    "        maxPixels=1e13 \n",
    "    )\n",
    "    export_task.start()\n",
    "    export_task.status()\n",
    "    print(\"----\", \"Flood image done\", \"----\", \"\\n\")\n",
    "\n",
    "    # Get soil moisture data\n",
    "    print(\"----\", \"Gathering soil moisture images\", \"----\")\n",
    "    soil_moisture_date = pd.to_datetime(date) - timedelta(days=1)\n",
    "    soil_moisture_file = pull_soil_moisture(soil_moisture_date, 'data/soil_moisture')\n",
    "    send_to_google_drive(soil_moisture_file)\n",
    "    print(\"----\", \"Soil moisture image done\", \"----\", \"\\n\")\n",
    "    \n",
    "\n",
    "    # Pull rain data 2 days preceding, 1 day 'forecast'\n",
    "    print(\"----\", \"Gathering rain images\", \"----\")\n",
    "    timestamps = generate_timestamps(date, rainfall_days_before, rainfall_days_after)\n",
    "    files = generate_files_of_interest(drive, timestamps, 'config.json', 'MSWEP_Past_3hr_folder_id')\n",
    "\n",
    "    for i, timestamp in enumerate(timestamps):\n",
    "        print(\"Rain image \", i)\n",
    "        rainfall_file = pull_and_crop_rainfall_data(drive, files[i], timestamp, \"./data/temp/\", bangladesh_shape)\n",
    "        send_to_google_drive(drive, rainfall_file, 'config.json', 'bangladesh_rainfall_folder_id')\n",
    "        \n",
    "    print(\"----\", \"Rain images done\", \"----\")\n",
    "\n",
    "\n",
    "    print(\"============\")    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_non_flood_event(date):\n",
    "    # print(\"============\")\n",
    "    # print(\"Working on the non-flood event on:\", date)\n",
    "    \n",
    "    # Get permanent water\n",
    "    # print(\"----\", \"Generating water image\", \"----\")\n",
    "    water_overlay = ee.Image(perm_water.filterMetadata('month', 'equals', date.month).first())\n",
    "    occurrence_band = water_overlay.select('monthly_recurrence')\n",
    "    water_overlay_filtered = occurrence_band.gt(perm_water_threshold) \n",
    "    water_overlay_filtered = water_overlay_filtered.rename('flooded')\n",
    "\n",
    "    export_task = ee.batch.Export.image.toDrive(\n",
    "        image=water_overlay_filtered,\n",
    "        description=f'BangladeshWater{date.year}{date.month:02}{date.day:02}',\n",
    "        folder='EarthEngineExports/BangladeshNonFloodImages',\n",
    "        fileNamePrefix=f'BangladeshWater{date.year}{date.month:02}{date.day:02}',\n",
    "        region=bangladesh_shape_ee.getInfo()['coordinates'],\n",
    "        scale=250,  # Adjust the scale as needed\n",
    "        maxPixels=1e13 \n",
    "    )\n",
    "    export_task.start()\n",
    "    export_task.status()\n",
    "    # print(\"----\", \"Flood image done\", \"----\", \"\\n\")\n",
    "\n",
    "    # Get soil moisture data\n",
    "    # print(\"----\", \"Gathering soil moisture images\", \"----\")\n",
    "    soil_moisture_date = pd.to_datetime(date) - timedelta(days=1)\n",
    "    soil_moisture_file = pull_soil_moisture(soil_moisture_date, 'data/soil_moisture')\n",
    "    send_to_google_drive(soil_moisture_file)\n",
    "    # print(\"----\", \"Soil moisture image done\", \"----\", \"\\n\")\n",
    "\n",
    "    # Get rain data\n",
    "    # print(\"----\", \"Gathering rain images\", \"----\")\n",
    "    timestamps = generate_timestamps(date, rainfall_days_before, rainfall_days_after)\n",
    "    files = generate_files_of_interest(drive, timestamps, 'config.json', 'MSWEP_Past_3hr_folder_id')\n",
    "\n",
    "    for i, timestamp in enumerate(timestamps):\n",
    "        # print(\"Rain image \", i)\n",
    "        rainfall_file = pull_and_crop_rainfall_data(drive, files[i], timestamp, \"./data/temp/\", bangladesh_shape)\n",
    "        send_to_google_drive(drive, rainfall_file, 'config.json', 'bangladesh_rainfall_folder_id')\n",
    "    \n",
    "    # print(\"----\", \"Rain images done\", \"----\")\n",
    "\n",
    "    # print(\"============\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_dates_to_generate = 100\n",
    "safety_window = 10\n",
    "random_dates = generate_random_non_flood_dates('Bangladesh_Flood_Events.xlsx', num_dates_to_generate, safety_window)\n",
    "\n",
    "# Submit jobs, thread pool as I/O bound processes i.e. the API requests are the biggest limiting factor\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    futures = [executor.submit(process_non_flood_event, date) for date in random_dates]\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        print(future.result())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
