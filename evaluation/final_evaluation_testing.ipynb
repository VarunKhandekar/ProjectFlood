{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/usr/lib/python310.zip', '/usr/lib/python3.10', '/usr/lib/python3.10/lib-dynload', '', '/vol/bitbucket/vk223/project_flood/projectfloodvenv/lib/python3.10/site-packages', '/homes/vk223/ProjectFlood', '/home/vkhandekar/project_flood']\n",
      "/homes/vk223/ProjectFlood/evaluation\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "paths_to_add = [\"/homes/vk223/ProjectFlood\", \"/home/vkhandekar/project_flood\"]\n",
    "for p in paths_to_add:\n",
    "    if p not in sys.path:\n",
    "        sys.path.append(p)\n",
    "print(sys.path)\n",
    "\n",
    "from model_runs.model_run_helpers import *\n",
    "from model_runs.distributed_gpu_helpers import *\n",
    "from visualisations.visualisation_helpers import *\n",
    "import os\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/homes/vk223/ProjectFlood/static/imperial_data_paths.json\n"
     ]
    }
   ],
   "source": [
    "from evaluation.model_evaluation_helpers import *\n",
    "os.environ[\"PROJECT_FLOOD_DATA\"] = \"/homes/vk223/ProjectFlood/static/imperial_data_paths.json\"\n",
    "os.environ[\"PROJECT_FLOOD_CORE_PATHS\"] = \"/homes/vk223/ProjectFlood/static/imperial_core_paths.json\"\n",
    "print(os.environ[\"PROJECT_FLOOD_DATA\"])\n",
    "resolution = 256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['epochs', 'batchsize', 'lr', 'precedingrainfall', 'dropout', 'outputchannels', 'convblocklayers', 'convLSTMlayers', 'optim', 'criterion', 'transforms', 'res'])\n",
      "dict_keys(['epochs', 'batchsize', 'lr', 'precedingrainfall', 'dropout', 'outputchannels', 'convblocklayers', 'convLSTMlayers', 'optim', 'criterion', 'transforms', 'res'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homes/vk223/ProjectFlood/evaluation/model_evaluation_helpers.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filepath)\n",
      "/vol/bitbucket/vk223/project_flood/projectfloodvenv/lib/python3.10/site-packages/torch/cuda/__init__.py:205: UserWarning: \n",
      "    Found GPU0 NVIDIA GeForce GT 730 which is of cuda capability 3.5.\n",
      "    PyTorch no longer supports this GPU because it is too old.\n",
      "    The minimum cuda capability supported by this library is 3.7.\n",
      "    \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# print(device)\n",
    "torch.manual_seed(42)\n",
    "# Load newly trained best, final models\n",
    "final_sep_branch_model, _, _, final_sep_branch_params = load_checkpoint(\"/homes/vk223/ProjectFlood/models/saved_models/ConvLSTMMerged_epochs500_batchsize16_lr0p001_precedingrainfall1_dropout0p3_outputchannels8_convblocklayers2_convLSTMlayers2_optimRMSprop_criterionBCELoss_transformsFalse_res256_20240824_500.pt\")\n",
    "final_merged_model, _, _, final_merged_params = load_checkpoint(\"/homes/vk223/ProjectFlood/models/saved_models/ConvLSTMSeparateBranches_epochs500_batchsize32_lr0p001_precedingrainfall1_dropout0p5_outputchannels8_convblocklayers2_convLSTMlayers1_optimRMSprop_criterionBCELoss_transformsFalse_res256_20240824_500.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sep_branch_test_dataloader = get_dataloader(\"test_labels_path\", resolution=256, preceding_rainfall_days=final_sep_branch_params['precedingrainfall'], forecast_rainfall_days=1, \n",
    "                                                transform=None, batch_size=16, shuffle=False, num_workers=4)\n",
    "    \n",
    "merged_test_dataloader = get_dataloader(\"test_labels_path\", resolution=256, preceding_rainfall_days=final_merged_params['precedingrainfall'], forecast_rainfall_days=1, \n",
    "                                            transform=None, batch_size=16, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualisations.visualisation_helpers import *\n",
    "# Plot image\n",
    "with torch.no_grad():\n",
    "    size = 3\n",
    "\n",
    "    sep_branch_flooded_images = []\n",
    "    sep_branch_non_flooded_images = []\n",
    "    for inputs, targets, flooded in sep_branch_test_dataloader:\n",
    "        sep_branch_outputs = final_sep_branch_model(inputs)\n",
    "        for i in range(len(flooded)):\n",
    "            if flooded[i] == 1 and len(sep_branch_flooded_images) < size:\n",
    "                sep_branch_flooded_images.append((sep_branch_outputs[i], targets[i], flooded[i]))\n",
    "            elif flooded[i] == 0 and len(sep_branch_non_flooded_images) < size:\n",
    "                sep_branch_non_flooded_images.append((sep_branch_outputs[i], targets[i], flooded[i]))\n",
    "            # Stop if we've collected 4 images in each category\n",
    "            if len(sep_branch_flooded_images) >= size and len(sep_branch_non_flooded_images) >= size:\n",
    "                break\n",
    "        if len(sep_branch_flooded_images) >= 4 and len(sep_branch_non_flooded_images) >= 4:\n",
    "            break\n",
    "\n",
    "    merged_flooded_images = []\n",
    "    merged_non_flooded_images = []\n",
    "    for inputs, targets, flooded in merged_test_dataloader:\n",
    "        merged_outputs = final_merged_model(inputs)\n",
    "        for i in range(len(flooded)):\n",
    "            if flooded[i] == 1 and len(merged_flooded_images) < size:\n",
    "                merged_flooded_images.append((merged_outputs[i], targets[i], flooded[i]))\n",
    "            elif flooded[i] == 0 and len(merged_non_flooded_images) < size:\n",
    "                merged_non_flooded_images.append((merged_outputs[i], targets[i], flooded[i]))\n",
    "            # Stop if we've collected 4 images in each category\n",
    "            if len(merged_flooded_images) >= size and len(merged_non_flooded_images) >= size:\n",
    "                break\n",
    "        if len(merged_flooded_images) >= 4 and len(merged_non_flooded_images) >= 4:\n",
    "            break\n",
    "    \n",
    "    with open(os.environ[\"PROJECT_FLOOD_CORE_PATHS\"]) as core_config_file:\n",
    "        core_config = json.load(core_config_file)\n",
    "    dimension_string = core_config[f\"rainfall_reprojection_master_256\"]\n",
    "    match = re.search(r'_(\\d+)_(\\d+)\\.tif$', dimension_string)\n",
    "    new_dimension_right, new_dimension_bottom = int(match.group(1)), int(match.group(2))\n",
    "\n",
    "\n",
    "    selected_sep_branch_outputs = [img[0] for img in sep_branch_flooded_images + sep_branch_non_flooded_images]\n",
    "    selected_sep_branch_outputs = [i[:new_dimension_bottom, :new_dimension_right] for i in selected_sep_branch_outputs] #crop\n",
    "\n",
    "\n",
    "    selected_merged_outputs = [img[0] for img in merged_flooded_images + merged_non_flooded_images]\n",
    "    selected_merged_outputs = [i[:new_dimension_bottom, :new_dimension_right] for i in selected_merged_outputs] #crop\n",
    "\n",
    "    model_names = ['Branched', 'Merged']\n",
    "    selected_model_outputs = [selected_sep_branch_outputs, selected_merged_outputs]\n",
    "\n",
    "    selected_targets = [img[1] for img in sep_branch_flooded_images + sep_branch_non_flooded_images]\n",
    "    selected_targets = [i[:new_dimension_bottom, :new_dimension_right] for i in selected_targets] #crop\n",
    "\n",
    "\n",
    "    selected_targets_flooded = [img[2] for img in sep_branch_flooded_images + sep_branch_non_flooded_images] #boolean for flooded or not\n",
    "\n",
    "    # Do plotting\n",
    "    with open(os.environ[\"PROJECT_FLOOD_DATA\"]) as data_config_file:\n",
    "        data_config = json.load(data_config_file)\n",
    "    plot_filename = os.path.join(data_config[\"model_results_path\"], \"final_plots.png\")\n",
    "    plot_final_model_output_vs_label(model_names, selected_model_outputs, selected_targets, selected_targets_flooded, plot_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 203)\n"
     ]
    }
   ],
   "source": [
    "with open(os.environ[\"PROJECT_FLOOD_DATA\"]) as data_config_file:\n",
    "    data_config = json.load(data_config_file)\n",
    "with open(os.environ[\"PROJECT_FLOOD_CORE_PATHS\"]) as core_config_file:\n",
    "    core_config = json.load(core_config_file)\n",
    "dimension_string = core_config[f\"rainfall_reprojection_master_{resolution}\"]\n",
    "match = re.search(r'_(\\d+)_(\\d+)\\.tif$', dimension_string)\n",
    "new_dimension_right, new_dimension_bottom = int(match.group(1)), int(match.group(2))\n",
    "\n",
    "\n",
    "# model = final_sep_branch_model.to(device)\n",
    "model = final_sep_branch_model\n",
    "model.eval()\n",
    "criterion = getattr(torch.nn, final_sep_branch_params['criterion'])()\n",
    "\n",
    "# Load the True/False mask\n",
    "mask = np.load(os.path.join(data_config[\"model_results_path\"], \"perm_water_mask.npy\"))\n",
    "print(mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 256, 203])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/bitbucket/vk223/project_flood/projectfloodvenv/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:62: FutureWarning: Importing `PeakSignalNoiseRatio` from `torchmetrics` was deprecated and will be removed in 2.0. Import `PeakSignalNoiseRatio` from `torchmetrics.image` instead.\n",
      "  _future_warning(\n",
      "/vol/bitbucket/vk223/project_flood/projectfloodvenv/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:62: FutureWarning: Importing `StructuralSimilarityIndexMeasure` from `torchmetrics` was deprecated and will be removed in 2.0. Import `StructuralSimilarityIndexMeasure` from `torchmetrics.image` instead.\n",
      "  _future_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 256, 203])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/bitbucket/vk223/project_flood/projectfloodvenv/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:62: FutureWarning: Importing `PeakSignalNoiseRatio` from `torchmetrics` was deprecated and will be removed in 2.0. Import `PeakSignalNoiseRatio` from `torchmetrics.image` instead.\n",
      "  _future_warning(\n",
      "/vol/bitbucket/vk223/project_flood/projectfloodvenv/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:62: FutureWarning: Importing `StructuralSimilarityIndexMeasure` from `torchmetrics` was deprecated and will be removed in 2.0. Import `StructuralSimilarityIndexMeasure` from `torchmetrics.image` instead.\n",
      "  _future_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 256, 203])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/bitbucket/vk223/project_flood/projectfloodvenv/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:62: FutureWarning: Importing `PeakSignalNoiseRatio` from `torchmetrics` was deprecated and will be removed in 2.0. Import `PeakSignalNoiseRatio` from `torchmetrics.image` instead.\n",
      "  _future_warning(\n",
      "/vol/bitbucket/vk223/project_flood/projectfloodvenv/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:62: FutureWarning: Importing `StructuralSimilarityIndexMeasure` from `torchmetrics` was deprecated and will be removed in 2.0. Import `StructuralSimilarityIndexMeasure` from `torchmetrics.image` instead.\n",
      "  _future_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13, 256, 203])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/bitbucket/vk223/project_flood/projectfloodvenv/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:62: FutureWarning: Importing `PeakSignalNoiseRatio` from `torchmetrics` was deprecated and will be removed in 2.0. Import `PeakSignalNoiseRatio` from `torchmetrics.image` instead.\n",
      "  _future_warning(\n",
      "/vol/bitbucket/vk223/project_flood/projectfloodvenv/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:62: FutureWarning: Importing `StructuralSimilarityIndexMeasure` from `torchmetrics` was deprecated and will be removed in 2.0. Import `StructuralSimilarityIndexMeasure` from `torchmetrics.image` instead.\n",
      "  _future_warning(\n"
     ]
    }
   ],
   "source": [
    "thresholds = np.arange(0, 1.05, 0.05)\n",
    "confusion_matrices = {thr: np.zeros((2, 2)) for thr in thresholds}\n",
    "precision_scores = {thr: 0 for thr in thresholds}\n",
    "recall_scores = {thr: 0 for thr in thresholds}\n",
    "accuracy_scores = {thr: 0 for thr in thresholds}\n",
    "\n",
    "total_samples = 0\n",
    "\n",
    "total_loss = 0\n",
    "total_rmse = 0\n",
    "total_mae = 0\n",
    "total_psnr = 0\n",
    "total_ssim = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels, _ in sep_branch_test_dataloader: #batch size is first dim. (BXY)\n",
    "        # inputs, labels = inputs.to(device, dtype=torch.float32), labels.to(device, dtype=torch.float32)\n",
    "        inputs, labels = inputs.to(torch.float32), labels.to(torch.float32)\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Crop the outputs and labels as required\n",
    "        cropped_outputs = outputs[..., 0:new_dimension_bottom, 0:new_dimension_right]  \n",
    "        cropped_labels = labels[..., 0:new_dimension_bottom, 0:new_dimension_right]\n",
    "        # print(cropped_outputs.shape)\n",
    "\n",
    "        # Apply mask (retain False values)\n",
    "        mask_torch = torch.from_numpy(mask)\n",
    "        mask_torch = mask_torch.unsqueeze(0)\n",
    "        mask_torch = mask_torch.expand(cropped_outputs.size(0), -1, -1)\n",
    "\n",
    "        masked_outputs = cropped_outputs[~mask_torch]\n",
    "        masked_labels = cropped_labels[~mask_torch]\n",
    "\n",
    "        # 1. Calculate PSNR and SSIM on cropped (but not masked) images\n",
    "        psnr_value = PSNR(cropped_outputs, cropped_labels)\n",
    "        ssim_value = SSIM(cropped_outputs, cropped_labels)\n",
    "        total_psnr += psnr_value.item()\n",
    "        total_ssim += ssim_value.item()\n",
    "\n",
    "        # 2. Calculate RMSE and MAE on masked images\n",
    "        rmse_loss = RMSELoss(masked_outputs, masked_labels)\n",
    "        mae_loss = MAELoss(masked_outputs, masked_labels)\n",
    "\n",
    "        total_rmse += rmse_loss.item()\n",
    "        total_mae += mae_loss.item()\n",
    "\n",
    "        # Calculate the total loss\n",
    "        loss = criterion(masked_outputs, masked_labels).item()\n",
    "        total_loss += loss\n",
    "        total_samples += masked_labels.size(0)\n",
    "\n",
    "        # Apply different thresholds and calculate confusion matrices\n",
    "        for thr in thresholds:\n",
    "            binary_output = (masked_outputs > thr).float()\n",
    "            binary_output = binary_output.cpu().numpy().flatten()\n",
    "            binary_labels = masked_labels.cpu().numpy().flatten()\n",
    "\n",
    "            cm = confusion_matrix(binary_labels, binary_output, labels=[0, 1])\n",
    "            confusion_matrices[thr] += cm\n",
    "            precision_scores[thr] += precision_score(binary_labels, binary_output, zero_division=0)\n",
    "            recall_scores[thr] += recall_score(binary_labels, binary_output, zero_division=0)\n",
    "\n",
    "            # Calculate accuracy at this threshold\n",
    "            correct_predictions = (binary_output == binary_labels).sum()\n",
    "            accuracy_scores[thr] += correct_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate averages\n",
    "avg_rmse = total_rmse / len(sep_branch_test_dataloader)\n",
    "avg_mae = total_mae / len(sep_branch_test_dataloader)\n",
    "avg_psnr = total_psnr / len(sep_branch_test_dataloader)\n",
    "avg_ssim = total_ssim / len(sep_branch_test_dataloader)\n",
    "average_loss = total_loss / total_samples\n",
    "\n",
    "# Calculate the final precision and recall\n",
    "for thr in thresholds:\n",
    "    precision_scores[thr] /= total_samples\n",
    "    recall_scores[thr] /= total_samples\n",
    "    accuracy_scores[thr] /= total_samples\n",
    "\n",
    "metric_accumulator = {\n",
    "    'average_rmse': avg_rmse,\n",
    "    'average_mae': avg_mae,\n",
    "    'average_psnr': avg_psnr,\n",
    "    'average_ssim': avg_ssim,\n",
    "    'average_loss': average_loss,\n",
    "    'confusion_matrices': confusion_matrices, #tn, fp, fn, tp\n",
    "    'precision_scores': precision_scores,\n",
    "    'recall_scores': recall_scores,\n",
    "    'accuracy_scores': accuracy_scores\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'average_rmse': 0.19500676915049553,\n",
       " 'average_mae': 0.27970991283655167,\n",
       " 'average_psnr': 14.693440914154053,\n",
       " 'average_ssim': 0.6088900566101074,\n",
       " 'average_loss': 2.1045159267313747e-07,\n",
       " 'confusion_matrices': {np.float64(0.0): array([[      0., 2344219.],\n",
       "         [      0.,   92365.]]),\n",
       "  np.float64(0.05): array([[1824063.,  520156.],\n",
       "         [   6616.,   85749.]]),\n",
       "  np.float64(0.1): array([[1999085.,  345134.],\n",
       "         [  12620.,   79745.]]),\n",
       "  np.float64(0.15000000000000002): array([[2081756.,  262463.],\n",
       "         [  18274.,   74091.]]),\n",
       "  np.float64(0.2): array([[2132962.,  211257.],\n",
       "         [  23113.,   69252.]]),\n",
       "  np.float64(0.25): array([[2169812.,  174407.],\n",
       "         [  27390.,   64975.]]),\n",
       "  np.float64(0.30000000000000004): array([[2196761.,  147458.],\n",
       "         [  31219.,   61146.]]),\n",
       "  np.float64(0.35000000000000003): array([[2217885.,  126334.],\n",
       "         [  34811.,   57554.]]),\n",
       "  np.float64(0.4): array([[2234717.,  109502.],\n",
       "         [  38226.,   54139.]]),\n",
       "  np.float64(0.45): array([[2248317.,   95902.],\n",
       "         [  41468.,   50897.]]),\n",
       "  np.float64(0.5): array([[2265257.,   78962.],\n",
       "         [  45969.,   46396.]]),\n",
       "  np.float64(0.55): array([[2275339.,   68880.],\n",
       "         [  49024.,   43341.]]),\n",
       "  np.float64(0.6000000000000001): array([[2285038.,   59181.],\n",
       "         [  52386.,   39979.]]),\n",
       "  np.float64(0.65): array([[2294590.,   49629.],\n",
       "         [  55914.,   36451.]]),\n",
       "  np.float64(0.7000000000000001): array([[2304087.,   40132.],\n",
       "         [  60015.,   32350.]]),\n",
       "  np.float64(0.75): array([[2313227.,   30992.],\n",
       "         [  65002.,   27363.]]),\n",
       "  np.float64(0.8): array([[2322004.,   22215.],\n",
       "         [  70325.,   22040.]]),\n",
       "  np.float64(0.8500000000000001): array([[2329892.,   14327.],\n",
       "         [  76252.,   16113.]]),\n",
       "  np.float64(0.9): array([[2337280.,    6939.],\n",
       "         [  82870.,    9495.]]),\n",
       "  np.float64(0.9500000000000001): array([[2.342575e+06, 1.644000e+03],\n",
       "         [8.934600e+04, 3.019000e+03]]),\n",
       "  np.float64(1.0): array([[2344219.,       0.],\n",
       "         [  92365.,       0.]])},\n",
       " 'precision_scores': {np.float64(0.0): np.float64(6.336757336879715e-08),\n",
       "  np.float64(0.05): np.float64(2.3200971182158708e-07),\n",
       "  np.float64(0.1): np.float64(3.078436242215067e-07),\n",
       "  np.float64(0.15000000000000002): np.float64(3.611725999050969e-07),\n",
       "  np.float64(0.2): np.float64(4.04892281300241e-07),\n",
       "  np.float64(0.25): np.float64(4.450243847446854e-07),\n",
       "  np.float64(0.30000000000000004): np.float64(4.805774028325148e-07),\n",
       "  np.float64(0.35000000000000003): np.float64(5.129975120810436e-07),\n",
       "  np.float64(0.4): np.float64(5.422927608360486e-07),\n",
       "  np.float64(0.45): np.float64(5.682918061577181e-07),\n",
       "  np.float64(0.5): np.float64(6.062144165845347e-07),\n",
       "  np.float64(0.55): np.float64(6.32408255819807e-07),\n",
       "  np.float64(0.6000000000000001): np.float64(6.600298987594007e-07),\n",
       "  np.float64(0.65): np.float64(6.928256035343577e-07),\n",
       "  np.float64(0.7000000000000001): np.float64(7.297520916160238e-07),\n",
       "  np.float64(0.75): np.float64(7.6619570620041e-07),\n",
       "  np.float64(0.8): np.float64(8.125126648778201e-07),\n",
       "  np.float64(0.8500000000000001): np.float64(8.63718432601284e-07),\n",
       "  np.float64(0.9): np.float64(9.433875772774577e-07),\n",
       "  np.float64(0.9500000000000001): np.float64(1.0576935083025437e-06),\n",
       "  np.float64(1.0): np.float64(0.0)},\n",
       " 'recall_scores': {np.float64(0.0): np.float64(1.6416425618817164e-06),\n",
       "  np.float64(0.05): np.float64(1.5268737915537328e-06),\n",
       "  np.float64(0.1): np.float64(1.4217315790691384e-06),\n",
       "  np.float64(0.15000000000000002): np.float64(1.3226795630555545e-06),\n",
       "  np.float64(0.2): np.float64(1.236653562670924e-06),\n",
       "  np.float64(0.25): np.float64(1.160834220642872e-06),\n",
       "  np.float64(0.30000000000000004): np.float64(1.0926263625622891e-06),\n",
       "  np.float64(0.35000000000000003): np.float64(1.0287101624898745e-06),\n",
       "  np.float64(0.4): np.float64(9.684140392627946e-07),\n",
       "  np.float64(0.45): np.float64(9.106429756454222e-07),\n",
       "  np.float64(0.5): np.float64(8.299952981620557e-07),\n",
       "  np.float64(0.55): np.float64(7.750861806077442e-07),\n",
       "  np.float64(0.6000000000000001): np.float64(7.148315754515697e-07),\n",
       "  np.float64(0.65): np.float64(6.518698118882166e-07),\n",
       "  np.float64(0.7000000000000001): np.float64(5.787539740520166e-07),\n",
       "  np.float64(0.75): np.float64(4.897540655372778e-07),\n",
       "  np.float64(0.8): np.float64(3.944911270976733e-07),\n",
       "  np.float64(0.8500000000000001): np.float64(2.8884297566177967e-07),\n",
       "  np.float64(0.9): np.float64(1.7116407307976603e-07),\n",
       "  np.float64(0.9500000000000001): np.float64(5.4948700110944245e-08),\n",
       "  np.float64(1.0): np.float64(0.0)},\n",
       " 'accuracy_scores': {np.float64(0.0): np.float64(0.03790757880705118),\n",
       "  np.float64(0.05): np.float64(0.7838071660981111),\n",
       "  np.float64(0.1): np.float64(0.8531739517291421),\n",
       "  np.float64(0.15000000000000002): np.float64(0.8847825480262531),\n",
       "  np.float64(0.2): np.float64(0.9038120581929455),\n",
       "  np.float64(0.25): np.float64(0.9171803639849888),\n",
       "  np.float64(0.30000000000000004): np.float64(0.9266690579926652),\n",
       "  np.float64(0.35000000000000003): np.float64(0.9338643773413927),\n",
       "  np.float64(0.4): np.float64(0.9393708569045844),\n",
       "  np.float64(0.45): np.float64(0.9436218903185771),\n",
       "  np.float64(0.5): np.float64(0.9487269882753888),\n",
       "  np.float64(0.55): np.float64(0.9516109438459746),\n",
       "  np.float64(0.6000000000000001): np.float64(0.9542117160746356),\n",
       "  np.float64(0.65): np.float64(0.9566840297728295),\n",
       "  np.float64(0.7000000000000001): np.float64(0.9588986055888079),\n",
       "  np.float64(0.75): np.float64(0.9606030409786817),\n",
       "  np.float64(0.8): np.float64(0.9620205993308665),\n",
       "  np.float64(0.8500000000000001): np.float64(0.962825414596829),\n",
       "  np.float64(0.9): np.float64(0.9631414307899913),\n",
       "  np.float64(0.9500000000000001): np.float64(0.9626567358235957),\n",
       "  np.float64(1.0): np.float64(0.9620924211929488)}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_accumulator"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projectfloodvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
